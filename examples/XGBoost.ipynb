{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4da4c4",
   "metadata": {},
   "source": [
    "# XGBoost Tutorial\n",
    "\n",
    "XGBoost is a common tree-based ensemble machine learning algorithm that uses a gradient boosting framework.\n",
    "It is known for its performance and speed in various machine learning tasks, especially in structured/tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122be18d",
   "metadata": {},
   "source": [
    "## Attempt 1\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a925aa6",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_stata(\"data/raw/usa_00005.dta\")\n",
    "\n",
    "y = data['inctot']\n",
    "\n",
    "# all other variables are features\n",
    "X = data.drop(columns=['inctot'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb93793",
   "metadata": {},
   "source": [
    "### Prep Data for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# one hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=False)\n",
    "print(X_encoded.shape)\n",
    "\n",
    "# get rid of columns with almost all 0's\n",
    "cols_to_keep = [col for col in X_encoded.columns if (X_encoded[col] != 0).mean() > 0.01]\n",
    "X_encoded = X_encoded[cols_to_keep]\n",
    "\n",
    "print(X_encoded.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55ba3a",
   "metadata": {},
   "source": [
    "### Define XGBoost Model Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b37c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 100,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'verbosity': 2\n",
    "}\n",
    "\n",
    "evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "num_round = 10000\n",
    "early_stopping_rounds = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aef6f3",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3252e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(params,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                early_stopping_rounds=early_stopping_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5921588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing target 9999999\n",
    "mask = (y != 9999999) & (y > 0)\n",
    "X = X[mask]\n",
    "y = y[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b2075",
   "metadata": {},
   "source": [
    "## Attempt 2\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e63a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_stata(\"data/raw/usa_00005.dta\")\n",
    "\n",
    "# subset to 10% of the data\n",
    "data = data.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "y = data['inctot']\n",
    "\n",
    "# all other variables are features\n",
    "X = data.drop(columns=['inctot'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e445ac",
   "metadata": {},
   "source": [
    "### Prep Data for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0095dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# one hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=False)\n",
    "print(X_encoded.shape)\n",
    "\n",
    "# get rid of columns with almost all 0's\n",
    "cols_to_keep = [col for col in X_encoded.columns if (X_encoded[col] != 0).mean() > 0.01]\n",
    "X_encoded = X_encoded[cols_to_keep]\n",
    "\n",
    "print(X_encoded.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9c13e",
   "metadata": {},
   "source": [
    "### Define XGBoost Model Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 1,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 100,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'verbosity': 2\n",
    "}\n",
    "\n",
    "evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "num_round = 10000\n",
    "early_stopping_rounds = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf668d96",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(params,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                early_stopping_rounds=early_stopping_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9c27e",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_plt = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))\n",
    "y_test_plt = y_test\n",
    "\n",
    "# y_plt_pred = np.log(y_pred)\n",
    "# y_plt_test = np.log(y_test)\n",
    "\n",
    "plt.scatter(y_test_plt, y_pred_plt, alpha=0.1)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"XGBoost: Predicted vs Actual Income\")\n",
    "plt.plot([y_test_plt.min(), y_test_plt.max()], [y_test_plt.min(), y_test_plt.max()], 'r--')  # diagonal line\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of actual values\n",
    "plt.hist(y_test, bins=50, alpha=0.7, color='blue')\n",
    "plt.xlabel(\"Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Actual Income Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d6a2c",
   "metadata": {},
   "source": [
    "### Attempt 3\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ef54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_stata(\"data/raw/usa_00005.dta\")\n",
    "\n",
    "# subset to 10% of the data\n",
    "data = data.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "y = data['inctot']\n",
    "\n",
    "# all other variables are features\n",
    "X = data.drop(columns=['inctot'])\n",
    "\n",
    "missing_val = 9999999\n",
    "mask = (y != missing_val)\n",
    "X = X[mask]\n",
    "y = y[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a314cae",
   "metadata": {},
   "source": [
    "### Prep Data for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca616189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# one hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=False)\n",
    "print(X_encoded.shape)\n",
    "\n",
    "# get rid of columns with almost all 0's\n",
    "cols_to_keep = [col for col in X_encoded.columns if (X_encoded[col] != 0).mean() > 0.01]\n",
    "X_encoded = X_encoded[cols_to_keep]\n",
    "\n",
    "print(X_encoded.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734d8f6",
   "metadata": {},
   "source": [
    "### Define XGBoost Model Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': 1,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 100,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'verbosity': 2\n",
    "}\n",
    "\n",
    "evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "num_round = 10000\n",
    "early_stopping_rounds = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b056c",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(params,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                early_stopping_rounds=early_stopping_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b72f9",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_plt = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))\n",
    "y_test_plt = y_test\n",
    "\n",
    "# y_plt_pred = np.log(y_pred)\n",
    "# y_plt_test = np.log(y_test)\n",
    "\n",
    "plt.scatter(y_test_plt, y_pred_plt, alpha=0.1)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"XGBoost: Predicted vs Actual Income\")\n",
    "plt.plot([y_test_plt.min(), y_test_plt.max()], [y_test_plt.min(), y_test_plt.max()], 'r--')  # diagonal line\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c7b7e",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas = [0.01, 0.1, 0.5, 1]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for eta in etas:\n",
    "    params = {\n",
    "        'eta': eta,\n",
    "        'max_depth': 10,\n",
    "        'min_child_weight': 100,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'verbosity': 1\n",
    "    }\n",
    "\n",
    "    evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    num_round = 10000\n",
    "    early_stopping_rounds = 50\n",
    "\n",
    "    print(f\"Training with eta={eta}\")\n",
    "    bst = xgb.train(params,\n",
    "                    dtrain,\n",
    "                    num_round,\n",
    "                    evallist,\n",
    "                    early_stopping_rounds=early_stopping_rounds)\n",
    "    models[eta] = bst\n",
    "\n",
    "scores = [models[eta].best_score for eta in etas]\n",
    "plt.plot(etas, scores, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Eta')\n",
    "plt.ylabel('Best RMSE Score')\n",
    "plt.title('Effect of Eta on Model Performance')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
